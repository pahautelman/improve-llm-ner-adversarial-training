{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare conll2012_ontonotesv5 dataset for NER evaluation\n",
    "dataset: https://huggingface.co/datasets/conll2012_ontonotesv5\n",
    "\n",
    "``` text\n",
    "@inproceedings{pradhan-etal-2013-towards,\n",
    "    title = \"Towards Robust Linguistic Analysis using {O}nto{N}otes\",\n",
    "    author = {Pradhan, Sameer  and  \n",
    "        Moschitti, Alessandro  and  \n",
    "        Xue, Nianwen  and  \n",
    "        Ng, Hwee Tou  and  \n",
    "        Bj{\\\"o}rkelund, Anders  and  \n",
    "        Uryupina, Olga  and  \n",
    "        Zhang, Yuchen  and  \n",
    "        Zhong, Zhi},\n",
    "    booktitle = \"Proceedings of the Seventeenth Conference on Computational Natural Language Learning\",\n",
    "    month = aug,\n",
    "    year = \"2013\",\n",
    "    address = \"Sofia, Bulgaria\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://aclanthology.org/W13-3516\",\n",
    "    pages = \"143--152\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.conda/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/lib/python3.11/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./.conda/lib/python3.11/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.conda/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./.conda/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in ./.conda/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.conda/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.conda/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in ./.conda/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./.conda/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in ./.conda/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in ./.conda/lib/python3.11/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in ./.conda/lib/python3.11/site-packages (from datasets) (0.20.2)\n",
      "Requirement already satisfied: packaging in ./.conda/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.conda/lib/python3.11/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pahautelman/Projects/AI-safety/improve-llm-ner-adversarial-training/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "import datasets\n",
    "import re\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMED_ENTITIES = [\n",
    "    \"O\",               # Outside of any named entity\n",
    "    \"B-PERSON\",        # Beginning of a person's name\n",
    "    \"I-PERSON\",        # Inside of a person's name (for multi-token names)\n",
    "    \"B-NORP\",          # Beginning of a nationalities or religious or political groups\n",
    "    \"I-NORP\",          # Inside of a NORP (for multi-token entities)\n",
    "    \"B-FAC\",           # Beginning of a facility (building, airport, etc.)\n",
    "    \"I-FAC\",           # Inside of a facility (for multi-token entities)\n",
    "    \"B-ORG\",           # Beginning of an organization's name\n",
    "    \"I-ORG\",           # Inside of an organization's name (for multi-token names)\n",
    "    \"B-GPE\",           # Beginning of a geopolitical entity (countries, cities, etc.)\n",
    "    \"I-GPE\",           # Inside of a GPE (for multi-token entities)\n",
    "    \"B-LOC\",           # Beginning of a location (not geopolitical)\n",
    "    \"I-LOC\",           # Inside of a location (for multi-token entities)\n",
    "    \"B-PRODUCT\",       # Beginning of a product name\n",
    "    \"I-PRODUCT\",       # Inside of a product name (for multi-token names)\n",
    "    \"B-DATE\",          # Beginning of a date\n",
    "    \"I-DATE\",          # Inside of a date (for multi-token entities)\n",
    "    \"B-TIME\",          # Beginning of a time expression\n",
    "    \"I-TIME\",          # Inside of a time expression (for multi-token entities)\n",
    "    \"B-PERCENT\",       # Beginning of a percentage\n",
    "    \"I-PERCENT\",       # Inside of a percentage (for multi-token entities)\n",
    "    \"B-MONEY\",         # Beginning of a monetary value\n",
    "    \"I-MONEY\",         # Inside of a monetary value (for multi-token entities)\n",
    "    \"B-QUANTITY\",      # Beginning of a quantity\n",
    "    \"I-QUANTITY\",      # Inside of a quantity (for multi-token entities)\n",
    "    \"B-ORDINAL\",       # Beginning of an ordinal number\n",
    "    \"I-ORDINAL\",       # Inside of an ordinal number (for multi-token entities)\n",
    "    \"B-CARDINAL\",      # Beginning of a cardinal number\n",
    "    \"I-CARDINAL\",      # Inside of a cardinal number (for multi-token entities)\n",
    "    \"B-EVENT\",         # Beginning of an event name\n",
    "    \"I-EVENT\",         # Inside of an event name (for multi-token names)\n",
    "    \"B-WORK_OF_ART\",   # Beginning of a work of art (books, songs, etc.)\n",
    "    \"I-WORK_OF_ART\",   # Inside of a work of art (for multi-token entities)\n",
    "    \"B-LAW\",           # Beginning of a law/legal reference\n",
    "    \"I-LAW\",           # Inside of a law/legal reference (for multi-token entities)\n",
    "    \"B-LANGUAGE\",      # Beginning of a language name\n",
    "    \"I-LANGUAGE\",      # Inside of a language name (for multi-token names)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"conll2012_ontonotesv5\"\n",
    "\n",
    "dataset = load_dataset(dataset_name, 'english_v12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "test_data = concatenate_datasets([dataset['test'], dataset['validation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to return data for evaluating the NER task\n",
    "# dataset columns:\n",
    "#  - sentence: sentence to be evaluated\n",
    "#  - ner_type: natural description of the entity\n",
    "#  - named_entity: correctly identified entity\n",
    "#  - sentence with annotated entity \n",
    "# the same sentence can appear appear multiple times with different entities\n",
    "# entity annotated like '@@NER##'\n",
    "# NER used for evaluation: \n",
    "#   - person name (1+2), \n",
    "#   - nationalities or religious or political groups (3+4), \n",
    "#   - facility (building, airport, etc.) (5+6),\n",
    "#   - organization's name (7+8),\n",
    "#   - geopolitical entity (countries, cities, etc.) (9+10),\n",
    "#   - location (not geopolitical) (11+12),\n",
    "#   - product name (13+14),\n",
    "#   - date (15+16),\n",
    "#   - time expression (17+18),\n",
    "#   - percentage (19+20),\n",
    "#   - monetary value (21+22),\n",
    "#   - quantity (23+24),\n",
    "#   - event name (29+30),\n",
    "#   - work of art (books, songs, etc.) (31+32),\n",
    "#   - law/legal reference (33+34),\n",
    "#   - language name (35+36)\n",
    "selected_ner = {\n",
    "    1: 'person',\n",
    "    3: 'nationalities or religious or political groups',\n",
    "    5: 'facility',\n",
    "    7: 'organization',\n",
    "    9: 'geopolitical entity',\n",
    "    11: 'location',\n",
    "    13: 'product',\n",
    "    15: 'date',\n",
    "    17: 'time expression',\n",
    "    19: 'percentage',\n",
    "    21: 'monetary value',\n",
    "    23: 'quantity',\n",
    "    29: 'event',\n",
    "    31: 'work of art',\n",
    "    33: 'law/legal reference',\n",
    "    35: 'language name' \n",
    "}\n",
    "\n",
    "def format_text(text: str) -> str:\n",
    "    formatted_text = re.sub(r'\\s*([.,;:!?%])\\s*', r'\\1 ', text)\n",
    "    formatted_text = re.sub(r'\\s*([-])\\s*', r'\\1', formatted_text)\n",
    "    formatted_text = re.sub(r'\\s+', ' ', formatted_text)\n",
    "    return formatted_text\n",
    "\n",
    "def extract_sentence(words: [str]) -> str:\n",
    "    text = ' '.join(words)\n",
    "    return format_text(text)\n",
    "\n",
    "def extract_named_entities(words: [str], ner_index: int, named_entities: [int]) -> [str]:\n",
    "    named_ents = []\n",
    "    index = 0\n",
    "\n",
    "    while index < len(words):\n",
    "        word, ner = words[index], named_entities[index]\n",
    "        if ner == ner_index:\n",
    "            named_ent = [word]\n",
    "            for next_word, next_ner in zip(words[index+1:], named_entities[index+1:]):\n",
    "                if next_ner == ner_index+1:\n",
    "                    named_ent.append(next_word)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            named_ents.append(format_text(' '.join(named_ent)))\n",
    "            index += len(named_ent)\n",
    "        else:\n",
    "            index += 1\n",
    "    return named_ents\n",
    "\n",
    "def extract_annotated_sentence(words: [str], ner_index: int, named_entities: [int]) -> str:\n",
    "    text = ''\n",
    "    index = 0\n",
    "\n",
    "    while index < len(words):\n",
    "        word, ner = words[index], named_entities[index]\n",
    "        if ner == ner_index:\n",
    "            named_ent = [word]\n",
    "            for next_word, next_ner in zip(words[index+1:], named_entities[index+1:]):\n",
    "                if next_ner == ner_index+1:\n",
    "                    named_ent.append(next_word)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            ent = ' @@' + ' '.join(named_ent) + '## '\n",
    "            text += ent\n",
    "            index += len(named_ent)\n",
    "        else:\n",
    "            text += word + ' '\n",
    "            index += 1\n",
    "    return format_text(text)\n",
    "\n",
    "def preprocess_dataset(data: datasets.arrow_dataset.Dataset):\n",
    "    sentences = []\n",
    "    ner_type = []\n",
    "    named_entities = [] # holds lists of named entities\n",
    "    annotated_sentences = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        sents = data[i]['sentences']\n",
    "        for sent in sents:\n",
    "            for ner_index, ner in selected_ner.items():\n",
    "                if ner_index in sent['named_entities']:\n",
    "                    text = extract_sentence(sent['words'])\n",
    "                    named_ents = extract_named_entities(sent['words'], ner_index, sent['named_entities'])\n",
    "                    annotated_text = extract_annotated_sentence(sent['words'], ner_index, sent['named_entities'])\n",
    "\n",
    "                    sentences.append(text)\n",
    "                    ner_type.append(ner)\n",
    "                    named_entities.append(named_ents)\n",
    "                    annotated_sentences.append(annotated_text)\n",
    "\n",
    "    # return Dataset\n",
    "    return datasets.Dataset.from_dict({\n",
    "        'sentence': sentences,\n",
    "        'ner_type': ner_type,\n",
    "        'named_entity': named_entities,\n",
    "        'sentence with annotated entity': annotated_sentences\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hundred Regiments Offensive was the campaign of the largest scale launched by the Eighth Route Army during the War of Resistance against Japan. \n",
      "['The Hundred Regiments Offensive', 'the War of Resistance against Japan']\n",
      " @@The Hundred Regiments Offensive## was the campaign of the largest scale launched by the Eighth Route Army during @@the War of Resistance against Japan##. \n"
     ]
    }
   ],
   "source": [
    "words = [\n",
    "    'The',\n",
    "    'Hundred',\n",
    "    'Regiments',\n",
    "    'Offensive',\n",
    "    'was',\n",
    "    'the',\n",
    "    'campaign',\n",
    "    'of',\n",
    "    'the',\n",
    "    'largest',\n",
    "    'scale',\n",
    "    'launched',\n",
    "    'by',\n",
    "    'the',\n",
    "    'Eighth',\n",
    "    'Route',\n",
    "    'Army',\n",
    "    'during',\n",
    "    'the',\n",
    "    'War',\n",
    "    'of',\n",
    "    'Resistance',\n",
    "    'against',\n",
    "    'Japan',\n",
    "    '.'\n",
    "]\n",
    "\n",
    "named_ents = [\n",
    "    29,\n",
    "    30,\n",
    "    30,\n",
    "    30,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    7,\n",
    "    8,\n",
    "    8,\n",
    "    8,\n",
    "    0,\n",
    "    29,\n",
    "    30,\n",
    "    30,\n",
    "    30,\n",
    "    30,\n",
    "    30,\n",
    "    0\n",
    "]\n",
    "\n",
    "print(extract_sentence(words))\n",
    "print(extract_named_entities(words, 29, named_ents))\n",
    "print(extract_annotated_sentence(words, 29, named_ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess_dataset(train_data)\n",
    "test = preprocess_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87265\n",
      "22132\n"
     ]
    }
   ],
   "source": [
    "print(train.shape[0])\n",
    "print(test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/87265 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 87265/87265 [00:00<00:00, 568388.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 22132/22132 [00:00<00:00, 472036.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train.save_to_disk('./data/train')\n",
    "test.save_to_disk('./data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87265\n",
      "22132\n"
     ]
    }
   ],
   "source": [
    "train = datasets.load_from_disk('./data/train')\n",
    "test = datasets.load_from_disk('./data/test')\n",
    "\n",
    "print(train.shape[0])\n",
    "print(test.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
